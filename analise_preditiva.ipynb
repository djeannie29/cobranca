{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Esse primeiro bloco faz a instalaÃ§Ã£o das bibliotecas necessÃ¡rias para o treinamento de mÃ¡quina e deve ser o primeiro a ser clicado."
      ],
      "metadata": {
        "id": "VSCrvP4Um1hg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyrYyyCWj7nN"
      },
      "outputs": [],
      "source": [
        "# ImportaÃ§Ã£o das bibliotecas necessÃ¡rias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import transformers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.combine import SMOTETomek\n",
        "from collections import Counter\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input, LSTM, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report\n",
        "import tensorflow.keras.backend as K\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“Œ **Carregar os dados**\n",
        "caminho_arquivo = \"/content/drive/MyDrive/Treinamento_maquina/aconamentos_2023_2024.xlsx\"\n",
        "df = pd.read_excel(caminho_arquivo)\n",
        "\n",
        "# ðŸ“Œ **VerificaÃ§Ã£o inicial**\n",
        "print(\"ðŸ“Š Dataset original:\")\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# ðŸ“Œ **CorreÃ§Ã£o de valores negativos no TEMPO_ATRASO**\n",
        "df.loc[df['TEMPO_ATRASO'] < 0, 'TEMPO_ATRASO'] = df['TEMPO_ATRASO'].median()\n",
        "\n",
        "# ðŸ“Œ **ConversÃ£o da Data**\n",
        "df['DATA'] = pd.to_datetime(df['Data'], errors='coerce')\n",
        "df['MES'] = df['DATA'].dt.month  # Extrair o mÃªs\n",
        "df['BIMESTRE'] = (df['DATA'].dt.month - 1) // 2 + 1  # Criar bimestres (1-6)\n",
        "df['SEMESTRE'] = (df['DATA'].dt.month - 1) // 6 + 1  # Criar semestres (1-2)\n",
        "\n",
        "# ðŸ“Œ **ConversÃ£o da Hora** (Corrigida para evitar remoÃ§Ã£o de todas as linhas)\n",
        "df['Hora'] = pd.to_datetime(df['Hora'], format='%H:%M:%S', errors='coerce')\n",
        "df['HORA_DECIMAL'] = df['Hora'].dt.hour + df['Hora'].dt.minute / 60\n",
        "\n",
        "# ðŸ“Œ **VerificaÃ§Ã£o apÃ³s conversÃ£o da Hora**\n",
        "if df['HORA_DECIMAL'].isna().sum() == len(df):\n",
        "    print(\"\\nðŸš¨ ERRO: Todas as horas falharam na conversÃ£o! Verifique o formato da coluna `Hora`.\")\n",
        "    print(df[['Hora']].head())\n",
        "    exit()\n",
        "\n",
        "# ðŸ“Œ **RepresentaÃ§Ã£o cÃ­clica da Hora**\n",
        "df['HORA_SIN'] = np.sin(2 * np.pi * df['HORA_DECIMAL'] / 24)\n",
        "df['HORA_COS'] = np.cos(2 * np.pi * df['HORA_DECIMAL'] / 24)\n",
        "\n",
        "# ðŸ“Œ **Criar novas variÃ¡veis**\n",
        "df['QTD_ACIONAMENTOS'] = df.groupby('COD')['COD'].transform('count')\n",
        "df['QTD_ACORDOS'] = df.groupby('COD')['ACAO'].transform(lambda x: (x.isin(['ACD', 'ACP'])).sum())\n",
        "df['ACAO_ANTERIOR'] = df.groupby('COD')['ACAO'].shift(1)\n",
        "df['DIAS_ENTRE_ACIONAMENTOS'] = df.groupby('COD')['DATA'].diff().dt.days\n",
        "df['MEDIA_DIAS_ACIONAMENTO'] = df.groupby('COD')['DIAS_ENTRE_ACIONAMENTOS'].transform('mean')\n",
        "\n",
        "# ðŸ“Œ **Criando Faixa de Valor da DÃ­vida**\n",
        "bins = [0, 10000, 50000, 100000, 500000, np.inf]\n",
        "labels = ['AtÃ© 10k', 'AtÃ© 50k', 'AtÃ© 100k', 'AtÃ© 500k', 'Acima 500k']\n",
        "df['FAIXA_VALOR_DIVIDA'] = pd.cut(df['VALOR'], bins=bins, labels=labels)\n",
        "\n",
        "# ðŸ“Œ **NÃºmero de acionamentos nos Ãºltimos 30 dias** (corrigido para evitar exclusÃ£o de linhas)\n",
        "df['ULTIMOS_30_DIAS'] = df.groupby('COD')['DATA'].transform(lambda x: x.diff().dt.days.fillna(999).le(30).sum())\n",
        "\n",
        "# ðŸ“Œ **PrescriÃ§Ã£o da DÃ­vida**\n",
        "df['PRESCRITA'] = (df['TEMPO_ATRASO'] > 1825).astype(int)\n",
        "\n",
        "# ðŸ“Œ **Tratar valores ausentes**\n",
        "df.fillna({'ACAO_ANTERIOR': 'SEM_ACAO',\n",
        "           'DIAS_ENTRE_ACIONAMENTOS': df['DIAS_ENTRE_ACIONAMENTOS'].median(),\n",
        "           'MEDIA_DIAS_ACIONAMENTO': df['MEDIA_DIAS_ACIONAMENTO'].median()}, inplace=True)\n",
        "\n",
        "# ðŸ”¹ **VerificaÃ§Ã£o de `ACAO` antes da conversÃ£o**\n",
        "print(\"\\nðŸ“Š Valores Ãºnicos em `ACAO` antes de transformar `y`:\")\n",
        "print(df['ACAO'].value_counts())\n",
        "\n",
        "# ðŸ”¹ **Encoding de variÃ¡veis categÃ³ricas**\n",
        "label_encoder = LabelEncoder()\n",
        "df['CREDOR'] = label_encoder.fit_transform(df['CREDOR'])\n",
        "df['TIPO_PESSOA'] = label_encoder.fit_transform(df['TIPO_PESSOA'])\n",
        "df['ACAO_ANTERIOR'] = label_encoder.fit_transform(df['ACAO_ANTERIOR'])\n",
        "df['FAIXA_VALOR_DIVIDA'] = label_encoder.fit_transform(df['FAIXA_VALOR_DIVIDA'])\n",
        "\n",
        "# ðŸ“Œ **Selecionando Features**\n",
        "features = ['CREDOR', 'TIPO_PESSOA', 'TEMPO_ATRASO', 'VALOR', 'QTD_ACIONAMENTOS', 'QTD_ACORDOS',\n",
        "            'MEDIA_DIAS_ACIONAMENTO', 'PRESCRITA', 'MES', 'BIMESTRE', 'SEMESTRE',\n",
        "            'FAIXA_VALOR_DIVIDA', 'HORA_SIN', 'HORA_COS', 'ULTIMOS_30_DIAS']\n",
        "\n",
        "X = df[features]\n",
        "y = df['ACAO'].apply(lambda x: 1 if x in ['ACD', 'ACP'] else 0)\n",
        "\n",
        "# ðŸ“Š **VerificaÃ§Ã£o do Dataset antes da divisÃ£o**\n",
        "print(f\"ðŸ“Š Tamanho do Dataset: {df.shape[0]} linhas\")\n",
        "print(f\"Tamanho de X: {X.shape}\")\n",
        "print(f\"Tamanho de y: {y.shape}\")\n",
        "print(f\"Valores Ãºnicos em `y`: {y.value_counts()}\")\n",
        "\n",
        "# ðŸš¨ **Se o DataFrame estiver vazio, parar a execuÃ§Ã£o antes da divisÃ£o Treino/Teste**\n",
        "if X.shape[0] == 0 or y.shape[0] == 0:\n",
        "    print(\"\\nðŸš¨ ERRO: O DataFrame estÃ¡ vazio! Verifique os dados antes da divisÃ£o Treino/Teste.\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# ðŸ“Œ AplicaÃ§Ã£o do SMOTE com menos oversampling (30% da classe majoritÃ¡ria)\n",
        "smote = SMOTE(sampling_strategy=0.3, k_neighbors=3, random_state=42)\n",
        "\n",
        "# ðŸ“Œ AplicaÃ§Ã£o do TomekLinks para undersampling (remover ruÃ­dos da classe majoritÃ¡ria)\n",
        "tomek = TomekLinks(sampling_strategy='majority')\n",
        "\n",
        "# ðŸ“Œ DivisÃ£o Treino/Teste (antes do SMOTE)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# ðŸ“Œ Aplicando o SMOTE primeiro para gerar exemplos sintÃ©ticos da classe minoritÃ¡ria\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# ðŸ“Œ Aplicando o TomekLinks para remover ruÃ­dos e exemplos muito prÃ³ximos da classe majoritÃ¡ria\n",
        "X_train_resampled, y_train_resampled = tomek.fit_resample(X_train_smote, y_train_smote)\n",
        "\n",
        "# ðŸ”¹ NormalizaÃ§Ã£o dos dados apÃ³s o resampling\n",
        "scaler = MinMaxScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ðŸ“Š Exibir o resultado final do balanceamento\n",
        "print(f\"ðŸ“Š ApÃ³s SMOTE-Tomek: Classe 0: {sum(y_train_resampled == 0)}, Classe 1: {sum(y_train_resampled == 1)}\")\n",
        "\n",
        "\n",
        "\n",
        "def focal_loss(alpha=0.25, gamma=2.0):\n",
        "    def loss(y_true, y_pred):\n",
        "        bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "        focal_loss = alpha * K.pow((1 - p_t), gamma) * bce\n",
        "        return K.mean(focal_loss)\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "Xrcr_Ddz75of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= ðŸ“Œ MLP - Multilayer Perceptron =======\n",
        "mlp_model = Sequential([\n",
        "    Input(shape=(X_train_scaled.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "mlp_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callback Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qAgUwP3BZuSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= ðŸ“Œ LSTM =======\n",
        "X_train_seq = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
        "X_test_seq = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
        "\n",
        "lstm_model = Sequential([\n",
        "    Input(shape=(1, X_train_scaled.shape[1])),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "_-EgEkb_Z0RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======= ðŸ“Œ Transformer =======\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.norm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dense = Dense(embed_dim, activation='relu')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        out1 = self.norm1(inputs + attn_output)\n",
        "        dense_output = self.dense(out1)\n",
        "        return self.norm2(out1 + dense_output)\n",
        "\n",
        "transformer_model = Sequential([\n",
        "    Input(shape=(1, X_train_scaled.shape[1])),\n",
        "    TransformerBlock(embed_dim=X_train_scaled.shape[1], num_heads=2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "transformer_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nM0mKRYiZ1UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_mlp = mlp_model.fit(X_train_scaled, y_train_resampled,\n",
        "                            epochs=100, batch_size=32,\n",
        "                            validation_data=(X_test_scaled, y_test),\n",
        "                            callbacks=[early_stopping])\n",
        "\n",
        "history_lstm = lstm_model.fit(X_train_seq, y_train_resampled,\n",
        "                              epochs=100, batch_size=32,\n",
        "                              validation_data=(X_test_seq, y_test),\n",
        "                              callbacks=[early_stopping])\n",
        "\n",
        "history_transformer = transformer_model.fit(X_train_seq, y_train_resampled,\n",
        "                                            epochs=100, batch_size=32,\n",
        "                                            validation_data=(X_test_seq, y_test),\n",
        "                                            callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "id": "t1xVAmRRiUSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ðŸ”¹ FunÃ§Ã£o para Plotar ComparaÃ§Ã£o do Treinamento\n",
        "def plot_training_comparison(history_mlp, history_lstm, history_transformer):\n",
        "    plt.figure(figsize=(14, 6))\n",
        "\n",
        "    # ðŸ”¹ ComparaÃ§Ã£o da Perda (Loss)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_mlp.history['loss'], label='MLP - Loss')\n",
        "    plt.plot(history_mlp.history['val_loss'], label='MLP - Val Loss', linestyle=\"dashed\")\n",
        "\n",
        "    plt.plot(history_lstm.history['loss'], label='LSTM - Loss')\n",
        "    plt.plot(history_lstm.history['val_loss'], label='LSTM - Val Loss', linestyle=\"dashed\")\n",
        "\n",
        "    plt.plot(history_transformer.history['loss'], label='Transformer - Loss')\n",
        "    plt.plot(history_transformer.history['val_loss'], label='Transformer - Val Loss', linestyle=\"dashed\")\n",
        "\n",
        "    plt.title('ComparaÃ§Ã£o da Perda (Loss)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # ðŸ”¹ ComparaÃ§Ã£o da AcurÃ¡cia\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_mlp.history['accuracy'], label='MLP - Acc')\n",
        "    plt.plot(history_mlp.history['val_accuracy'], label='MLP - Val Acc', linestyle=\"dashed\")\n",
        "\n",
        "    plt.plot(history_lstm.history['accuracy'], label='LSTM - Acc')\n",
        "    plt.plot(history_lstm.history['val_accuracy'], label='LSTM - Val Acc', linestyle=\"dashed\")\n",
        "\n",
        "    plt.plot(history_transformer.history['accuracy'], label='Transformer - Acc')\n",
        "    plt.plot(history_transformer.history['val_accuracy'], label='Transformer - Val Acc', linestyle=\"dashed\")\n",
        "\n",
        "    plt.title('ComparaÃ§Ã£o da AcurÃ¡cia')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('AcurÃ¡cia')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ðŸ”¹ Chamando a funÃ§Ã£o para plotar os grÃ¡ficos comparativos\n",
        "plot_training_comparison(history_mlp, history_lstm, history_transformer)\n"
      ],
      "metadata": {
        "id": "NE2T5O80inS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_training_comparison(history_mlp, history_lstm, history_transformer)\n"
      ],
      "metadata": {
        "id": "Eq1AegpdiZYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix, classification_report\n",
        "\n",
        "\n",
        "# ======= ðŸ“Œ AvaliaÃ§Ã£o =======\n",
        "models = {'MLP': mlp_model, 'LSTM': lstm_model, 'Transformer': transformer_model}\n",
        "thresholds = [0.15, 0.25]  # Testando diferentes thresholds\n",
        "\n",
        "for threshold in thresholds:\n",
        "    print(f\"\\nðŸ”¹ Testando Threshold: {threshold}\")\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        # ObtÃ©m as probabilidades de prediÃ§Ã£o\n",
        "        y_pred_probs = model.predict(X_test_scaled if model_name == 'MLP' else X_test_seq)\n",
        "        y_pred = (y_pred_probs > threshold).astype(\"int32\").flatten()  # Aplica o threshold\n",
        "\n",
        "        # Calcula mÃ©tricas de avaliaÃ§Ã£o\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, zero_division=1)\n",
        "        recall = recall_score(y_test, y_pred, zero_division=1)\n",
        "        precision = precision_score(y_test, y_pred, zero_division=1)\n",
        "\n",
        "        # Exibe os resultados\n",
        "        print(f\"\\nðŸ“Š {model_name} - Threshold {threshold}:\")\n",
        "        print(f\"ðŸ”¹ AcurÃ¡cia: {acc:.4f} | F1-Score: {f1:.4f} | Recall: {recall:.4f} | PrecisÃ£o: {precision:.4f}\")\n",
        "        print(f\"ðŸ”¹ Matriz de ConfusÃ£o:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "        print(f\"\\nðŸ“Œ RelatÃ³rio de ClassificaÃ§Ã£o:\\n{classification_report(y_test, y_pred, zero_division=1)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "weO_R3ZQZ8aD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}